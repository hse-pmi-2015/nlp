{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598\n",
      "598\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "df_corpus = pd.read_csv('corpus.csv',index_col=0)\n",
    "\n",
    "corpus = []\n",
    "for topic in df_corpus:\n",
    "    corpus += df_corpus[topic].tolist()\n",
    "\n",
    "answ_corpus = pd.Series(corpus)\n",
    "answ_corpus = answ_corpus.apply(strip_tags)\n",
    "answ_corpus = answ_corpus.apply(strip_multiple_whitespaces)\n",
    "corpus = pd.Series(corpus)\n",
    "corpus = corpus.apply(lambda x: x.lower())\n",
    "corpus = corpus.apply(strip_tags)\n",
    "corpus = corpus.apply(strip_punctuation2)\n",
    "corpus = corpus.apply(strip_numeric)\n",
    "corpus = corpus.apply(lambda x: re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', u' ', x))\n",
    "corpus = corpus.apply(lambda x: re.sub('«|»', ' ', x))\n",
    "corpus = corpus.apply(strip_multiple_whitespaces)\n",
    "corpus = corpus.apply(lambda x: x.partition(' – ')[2])\n",
    "\n",
    "\n",
    "\n",
    "import stop_words\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "corpus_tokens = []\n",
    "inds_to_drop = []\n",
    "\n",
    "for i, sentence in enumerate(corpus[:]):\n",
    "    tmp_tokens = []\n",
    "    sp = sentence.split()\n",
    "    for word in sp:\n",
    "        if word not in stop_words.get_stop_words('ru'):\n",
    "            if morph.word_is_known(word):\n",
    "                tmp_tokens.append(word)\n",
    "    if len(tmp_tokens) > 0:\n",
    "        corpus_tokens.append(tmp_tokens)\n",
    "    else:\n",
    "        inds_to_drop.append(i)\n",
    "#     break\n",
    "        \n",
    "print(len(corpus_tokens))#, len(texts))\n",
    "\n",
    "\n",
    "\n",
    "stemmer = RussianStemmer()\n",
    "\n",
    "corpus_tokens_stem = []\n",
    "\n",
    "for i, tokens in enumerate((corpus_tokens[:])):\n",
    "    tmp = [stemmer.stem(word) for word in tokens]\n",
    "    corpus_tokens_stem.append(tmp)\n",
    "#     break\n",
    "    \n",
    "print(len(corpus_tokens_stem))\n",
    "\n",
    "\n",
    "clear_corpus = []\n",
    "for token_list in corpus_tokens_stem:\n",
    "    if len(' '.join(token_list))<2:\n",
    "        print(token_list)\n",
    "    clear_corpus.append(' '.join(token_list))\n",
    "clear_corpus[0]\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(clear_corpus)\n",
    "X = vectorizer.transform(clear_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"мурманск\" + 0.006*\"област\" + 0.004*\"росс\" + 0.003*\"дел\" + 0.003*\"сообща\" + 0.003*\"летн\" + 0.003*\"нов\" + 0.003*\"мест\" + 0.003*\"рубл\" + 0.003*\"работ\"'),\n",
       " (1,\n",
       "  '0.012*\"мурманск\" + 0.007*\"автомобил\" + 0.005*\"област\" + 0.004*\"рубл\" + 0.004*\"работ\" + 0.004*\"росс\" + 0.004*\"дел\" + 0.004*\"водител\" + 0.003*\"жител\" + 0.003*\"рф\"'),\n",
       " (2,\n",
       "  '0.012*\"мурманск\" + 0.006*\"област\" + 0.004*\"проект\" + 0.004*\"работ\" + 0.004*\"водител\" + 0.003*\"автомобил\" + 0.003*\"программ\" + 0.003*\"декабр\" + 0.003*\"нов\" + 0.003*\"дел\"'),\n",
       " (3,\n",
       "  '0.012*\"мурманск\" + 0.005*\"област\" + 0.004*\"росс\" + 0.004*\"автомобил\" + 0.004*\"январ\" + 0.004*\"рубл\" + 0.003*\"работ\" + 0.003*\"город\" + 0.003*\"рф\" + 0.003*\"дел\"'),\n",
       " (4,\n",
       "  '0.012*\"мурманск\" + 0.005*\"област\" + 0.005*\"полиц\" + 0.004*\"жител\" + 0.004*\"нов\" + 0.004*\"работ\" + 0.003*\"рубл\" + 0.003*\"участник\" + 0.003*\"свобод\" + 0.003*\"автомобил\"'),\n",
       " (5,\n",
       "  '0.013*\"мурманск\" + 0.005*\"водител\" + 0.004*\"автомобил\" + 0.004*\"област\" + 0.004*\"работ\" + 0.004*\"дел\" + 0.004*\"летн\" + 0.004*\"уголовн\" + 0.003*\"полиц\" + 0.003*\"город\"')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(corpus_tokens_stem)\n",
    "X_bow = [dictionary.doc2bow(text) for text in corpus_tokens_stem]\n",
    "lda_gensim = gensim.models.ldamodel.LdaModel(X_bow, num_topics=N_TOPICS, id2word=dictionary, iterations=10)\n",
    "lda_gensim.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 6\n",
    "max_iteration = 500\n",
    "batch_size = 10\n",
    "evaluate_every=1\n",
    "verb=1\n",
    "learning_method='batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=N_TOPICS, max_iter=max_iteration, batch_size=batch_size, evaluate_every=evaluate_every, verbose=verb, \n",
    "                                        n_jobs=4, learning_method=learning_method)\n",
    "# .fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 500, perplexity: 60109.6052\n",
      "iteration: 2 of max_iter: 500, perplexity: 56885.9946\n",
      "iteration: 3 of max_iter: 500, perplexity: 52872.0977\n",
      "iteration: 4 of max_iter: 500, perplexity: 48138.7124\n",
      "iteration: 5 of max_iter: 500, perplexity: 42960.6708\n",
      "iteration: 6 of max_iter: 500, perplexity: 38576.6075\n",
      "iteration: 7 of max_iter: 500, perplexity: 34922.3327\n",
      "iteration: 8 of max_iter: 500, perplexity: 32293.9316\n",
      "iteration: 9 of max_iter: 500, perplexity: 30156.9182\n",
      "iteration: 10 of max_iter: 500, perplexity: 28659.6656\n",
      "iteration: 11 of max_iter: 500, perplexity: 27582.2955\n",
      "iteration: 12 of max_iter: 500, perplexity: 26780.9283\n",
      "iteration: 13 of max_iter: 500, perplexity: 26132.5747\n",
      "iteration: 14 of max_iter: 500, perplexity: 25546.8534\n",
      "iteration: 15 of max_iter: 500, perplexity: 25127.0340\n",
      "iteration: 16 of max_iter: 500, perplexity: 24814.8644\n",
      "iteration: 17 of max_iter: 500, perplexity: 24642.9543\n",
      "iteration: 18 of max_iter: 500, perplexity: 24489.9499\n",
      "iteration: 19 of max_iter: 500, perplexity: 24365.0982\n",
      "iteration: 20 of max_iter: 500, perplexity: 24302.7278\n",
      "iteration: 21 of max_iter: 500, perplexity: 24231.6381\n",
      "iteration: 22 of max_iter: 500, perplexity: 24184.1363\n",
      "iteration: 23 of max_iter: 500, perplexity: 24118.5380\n",
      "iteration: 24 of max_iter: 500, perplexity: 24042.1798\n",
      "iteration: 25 of max_iter: 500, perplexity: 23965.3040\n",
      "iteration: 26 of max_iter: 500, perplexity: 23912.0739\n",
      "iteration: 27 of max_iter: 500, perplexity: 23882.1271\n",
      "iteration: 28 of max_iter: 500, perplexity: 23847.3946\n",
      "iteration: 29 of max_iter: 500, perplexity: 23754.4080\n",
      "iteration: 30 of max_iter: 500, perplexity: 23703.2634\n",
      "iteration: 31 of max_iter: 500, perplexity: 23670.0085\n",
      "iteration: 32 of max_iter: 500, perplexity: 23630.7018\n",
      "iteration: 33 of max_iter: 500, perplexity: 23596.2185\n",
      "iteration: 34 of max_iter: 500, perplexity: 23551.8837\n",
      "iteration: 35 of max_iter: 500, perplexity: 23509.0876\n",
      "iteration: 36 of max_iter: 500, perplexity: 23486.6673\n",
      "iteration: 37 of max_iter: 500, perplexity: 23466.0479\n",
      "iteration: 38 of max_iter: 500, perplexity: 23434.3988\n",
      "iteration: 39 of max_iter: 500, perplexity: 23430.8664\n",
      "iteration: 40 of max_iter: 500, perplexity: 23424.4287\n",
      "iteration: 41 of max_iter: 500, perplexity: 23419.2459\n",
      "iteration: 42 of max_iter: 500, perplexity: 23412.6560\n",
      "iteration: 43 of max_iter: 500, perplexity: 23389.8811\n",
      "iteration: 44 of max_iter: 500, perplexity: 23388.4342\n",
      "iteration: 45 of max_iter: 500, perplexity: 23386.4066\n",
      "iteration: 46 of max_iter: 500, perplexity: 23384.1194\n",
      "iteration: 47 of max_iter: 500, perplexity: 23379.8044\n",
      "iteration: 48 of max_iter: 500, perplexity: 23369.0319\n",
      "iteration: 49 of max_iter: 500, perplexity: 23362.2248\n",
      "iteration: 50 of max_iter: 500, perplexity: 23355.5432\n",
      "iteration: 51 of max_iter: 500, perplexity: 23340.9509\n",
      "iteration: 52 of max_iter: 500, perplexity: 23321.5158\n",
      "iteration: 53 of max_iter: 500, perplexity: 23320.0447\n",
      "iteration: 54 of max_iter: 500, perplexity: 23314.7803\n",
      "iteration: 55 of max_iter: 500, perplexity: 23308.3885\n",
      "iteration: 56 of max_iter: 500, perplexity: 23305.4630\n",
      "iteration: 57 of max_iter: 500, perplexity: 23298.9374\n",
      "iteration: 58 of max_iter: 500, perplexity: 23297.7497\n",
      "iteration: 59 of max_iter: 500, perplexity: 23296.6300\n",
      "iteration: 60 of max_iter: 500, perplexity: 23296.0946\n",
      "iteration: 61 of max_iter: 500, perplexity: 23295.8705\n",
      "iteration: 62 of max_iter: 500, perplexity: 23295.8010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=10, doc_topic_prior=None,\n",
       "             evaluate_every=1, learning_decay=0.7, learning_method='batch',\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=500,\n",
       "             mean_change_tol=0.001, n_components=6, n_jobs=4,\n",
       "             n_topics=None, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "svd = TruncatedSVD(n_components=N_TOPICS, n_iter=100).fit(X)\n",
    "\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7109 7109 7109\n"
     ]
    }
   ],
   "source": [
    "keys = list(vectorizer.vocabulary_.keys())\n",
    "values = list(vectorizer.vocabulary_.values())\n",
    "\n",
    "_ind = np.argsort(values)\n",
    "words_sorted = np.asarray(keys)[_ind]\n",
    "\n",
    "print(len(keys), len(values), len(words_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['auto', 'crime', 'culture', 'education'], dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11974542 0.12172687 0.12689177 0.13078238 0.1311683  0.14097139\n",
      " 0.15137524 0.17164023 0.17526135 0.22917545]\n",
      "['композитор' 'географ' 'находк' 'свидан' 'сочинен' 'скакалк' 'итальянск'\n",
      " 'беллин' 'столов' 'диктант']\n",
      "\n",
      "[0.11542823 0.11580785 0.11583691 0.1189751  0.13569249 0.14079865\n",
      " 0.14775426 0.18384743 0.25467627 0.28934121]\n",
      "['жител' 'срок' 'уголовн' 'дел' 'рубл' 'свобод' 'лишен' 'област' 'полиц'\n",
      " 'мурманск']\n",
      "\n",
      "[0.10358589 0.1099553  0.11000508 0.11068345 0.11152459 0.1199679\n",
      " 0.1369217  0.15086606 0.15183271 0.1840259 ]\n",
      "['пленк' 'форт' 'зенитн' 'ракетн' 'топлив' 'кремлевск' 'литр' 'дизельн'\n",
      " 'уаз' 'копеек']\n",
      "\n",
      "[0.1060818  0.12212066 0.15457948 0.18857641 0.19580236]\n",
      "['незнаком' 'краб' 'куб' 'вход' 'свободн']\n",
      "\n",
      "[0.10332364 0.11979795 0.12549766 0.15889184 0.16770367 0.21908653\n",
      " 0.22107959 0.22728758 0.23558164]\n",
      "['кладбищ' 'поход' 'воздержа' 'вблиз' 'владимир' 'зон' 'солов' 'взрывн'\n",
      " 'радиус']\n",
      "\n",
      "[0.10151888 0.11014743 0.11992617 0.14006268 0.15721703 0.1602395\n",
      " 0.16370343 0.22057178 0.3714278 ]\n",
      "['результат' 'кол' 'пострада' 'дорог' 'происшеств' 'произошл' 'дтп'\n",
      " 'водител' 'автомобил']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "stub = [0] * N_TOPICS\n",
    "threshold = 0.1\n",
    "\n",
    "for i in range(N_TOPICS):\n",
    "    \n",
    "    szd = pd.Series(np.sort(svd.components_[i])[-10:])\n",
    "    szd_mask = szd.apply(lambda x: False if x<threshold else True)\n",
    "    \n",
    "    print(szd[szd_mask].values)\n",
    "    \n",
    "    ind = np.argsort(lda.components_[i])[-szd[szd_mask].shape[0]:]\n",
    "    print(words_sorted[ind])\n",
    "    \n",
    "    print()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
